# üîß H∆∞·ªõng d·∫´n Build IoT Analytics Pipeline T·ª´ ƒê·∫ßu

## üìã T·ªïng quan

Pipeline IoT n√†y x·ª≠ l√Ω d·ªØ li·ªáu nƒÉng l∆∞·ª£ng qua 3 layers:
- **Bronze**: Raw data ingestion
- **Silver**: Data cleaning & transformation  
- **Gold**: Business aggregations & analytics
- **Forecasting**: ML predictions v·ªõi 6 models

## üõ†Ô∏è Y√™u c·∫ßu h·ªá th·ªëng

### Software Requirements:
- Python 3.8+
- Java 8+ (cho PySpark)
- Power BI Desktop (cho visualization)

### Hardware Requirements:
- RAM: 8GB+ (16GB khuy·∫øn ngh·ªã)
- Storage: 5GB+ free space
- CPU: 4 cores+

## üìÅ C·∫•u tr√∫c Project

```
iot_pipeline/
‚îú‚îÄ‚îÄ src/                          # Source code
‚îÇ   ‚îú‚îÄ‚îÄ layers/                   # Data processing layers
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ bronze.py            # Raw data ingestion
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ silver.py            # Data cleaning
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gold.py              # Business aggregations
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ forecasting.py       # ML forecasting
‚îÇ   ‚îú‚îÄ‚îÄ utils/                   # Utility functions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ helpers.py           # Logging, validation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ transformations.py   # Data transformations
‚îÇ   ‚îî‚îÄ‚îÄ config/                  # Configuration
‚îÇ       ‚îú‚îÄ‚îÄ settings.py          # Environment settings
‚îÇ       ‚îî‚îÄ‚îÄ constants.py         # Pipeline constants
‚îú‚îÄ‚îÄ data/                        # Data storage
‚îÇ   ‚îú‚îÄ‚îÄ bronze/                  # Raw data
‚îÇ   ‚îú‚îÄ‚îÄ silver/                  # Clean data
‚îÇ   ‚îî‚îÄ‚îÄ gold/                    # Business data
‚îú‚îÄ‚îÄ main.py                      # Pipeline runner
‚îú‚îÄ‚îÄ requirements.txt             # Python dependencies
‚îî‚îÄ‚îÄ README.md                    # Project documentation
```

## üöÄ Setup Pipeline

### B∆∞·ªõc 1: T·∫°o m√¥i tr∆∞·ªùng ·∫£o

```bash
# T·∫°o virtual environment
python -m venv iot_env

# K√≠ch ho·∫°t (Windows)
.\iot_env\Scripts\Activate.ps1

# K√≠ch ho·∫°t (Linux/Mac)
source iot_env/bin/activate
```

### B∆∞·ªõc 2: C√†i ƒë·∫∑t dependencies

```bash
# C√†i ƒë·∫∑t Python packages
pip install -r requirements.txt

# N·∫øu thi·∫øu pyarrow cho Parquet support
pip install pyarrow
```

### B∆∞·ªõc 3: C·∫•u h√¨nh environment

```python
# src/config/settings.py
class Config:
    def __init__(self, environment="local"):
        self.environment = environment
        self.base_path = "data"
        
        # Table paths
        self.bronze_table = f"{self.base_path}/bronze/raw_data.parquet"
        self.silver_table = f"{self.base_path}/silver/clean_data.parquet"
        self.gold_daily_table = f"{self.base_path}/gold/daily_aggregates.parquet"
        self.gold_hourly_table = f"{self.base_path}/gold/hourly_aggregates.parquet"
```

## üìä Data Processing Layers

### Bronze Layer - Raw Data Ingestion

**File**: `src/layers/bronze.py`

**Ch·ª©c nƒÉng**:
- ƒê·ªçc raw CSV data
- Validate schema
- Add metadata (ingest_time, source_file)
- Basic data quality checks

**Key Methods**:
```python
def read_csv_data(self, input_path):
    """ƒê·ªçc CSV v·ªõi schema ƒë·ªãnh s·∫µn"""
    
def process_bronze_data(self, df):
    """X·ª≠ l√Ω raw data cho Bronze layer"""
    
def save_bronze_data(self, df, output_path):
    """L∆∞u data v√†o Bronze layer"""
```

### Silver Layer - Data Cleaning

**File**: `src/layers/silver.py`

**Ch·ª©c nƒÉng**:
- Clean date/time columns
- Add derived time features (year, month, day, hour)
- Calculate electrical features (power factor, energy)
- Handle outliers v√† missing values
- Impute data theo time patterns

**Key Transformations**:
```python
# Transformation pipeline
df_transformed = (df
    .transform(with_timestamp)           # T·∫°o timestamp
    .transform(with_time_columns)        # T·∫°o time features
    .transform(with_electric_features)   # T√≠nh electrical metrics
    .transform(null_out_of_range)        # X·ª≠ l√Ω outliers
    .transform(impute_by_time_median))   # Impute missing values
```

### Gold Layer - Business Aggregations

**File**: `src/layers/gold.py`

**Ch·ª©c nƒÉng**:
- T·∫°o hourly aggregations
- T·∫°o daily aggregations
- Calculate business KPIs
- Find peak consumption patterns

**Aggregations**:
```python
# Hourly aggregations
hourly_df = df.groupBy("date", "hour").agg(
    F.sum("energy_kWh").alias("energy_kWh"),
    F.avg("power_factor").alias("pf_avg"),
    F.sum("Sub_metering_1").alias("sm1_sum")
)

# Daily aggregations v·ªõi peak hour detection
daily_df = hourly_df.groupBy("date").agg(
    F.sum("energy_kWh").alias("daily_energy"),
    F.max("energy_kWh").alias("peak_energy")
)
```

## ü§ñ Machine Learning Forecasting

### Enhanced Forecasting Pipeline

**File**: `src/layers/forecasting.py`

**6 ML Models**:
1. **Naive Seasonal-7**: Baseline model (7-day pattern)
2. **SARIMAX**: Statistical time series model
3. **Prophet**: Facebook's forecasting tool
4. **XGBoost**: Gradient boosting (Best performer)
5. **LightGBM**: Microsoft's gradient boosting
6. **Ensemble**: Top-3 model combination

**Feature Engineering (19 features)**:
```python
def create_features(self, df):
    # Lag features
    for lag in [1, 2, 3, 7, 14]:
        df[f"lag_{lag}"] = df["energy_kWh"].shift(lag)
    
    # Rolling statistics
    for window in [3, 7, 14]:
        df[f"roll_mean_{window}"] = df["energy_kWh"].rolling(window).mean()
        df[f"roll_std_{window}"] = df["energy_kWh"].rolling(window).std()
    
    # Cyclical encoding
    df["dayofweek_sin"] = np.sin(2 * np.pi * df.index.dayofweek / 7)
    df["dayofweek_cos"] = np.cos(2 * np.pi * df.index.dayofweek / 7)
    df["month_sin"] = np.sin(2 * np.pi * df.index.month / 12)
    df["month_cos"] = np.cos(2 * np.pi * df.index.month / 12)
```

## üèÉ‚Äç‚ôÇÔ∏è Ch·∫°y Pipeline: Standalone Testing 
```bash

python test_enhanced_real.py

# K·∫øt qu·∫£: 6 ML models, feature engineering, performance metrics
# Output: enhanced_forecast_data.csv ready for Power BI
```


## üìà Performance Benchmarks

### Expected Results:
- **XGBoost**: RMSE=6.577, MAPE=20.61% (Best)
- **Naive Seasonal**: RMSE=6.583, MAPE=21.44%
- **Prophet**: RMSE=8.559, MAPE=29.23%

### Data Volume:
- **Bronze**: 2,075,259 rows (raw sensor data)
- **Silver**: 2,075,259 rows (cleaned data)
- **Gold Daily**: 1,442 rows (daily aggregates)
- **Gold Hourly**: 34,589 rows (hourly aggregates)
- **Forecasts**: 90 predictions (30 days √ó 3 models)

## üìä Power BI Integration

### Export Data
```bash
# Export t·∫•t c·∫£ data cho Power BI
python export_for_powerbi.py
```

### Files ƒë∆∞·ª£c t·∫°o:
- `bronze_raw_data.csv` - Raw sensor data
- `silver_clean_data.csv` - Clean data
- `gold_daily_aggregates.csv` - Daily KPIs
- `gold_hourly_aggregates.csv` - Hourly patterns
- `enhanced_forecast_data.csv` - ML predictions
- `DAX_Measures.txt` - Power BI formulas

### Import v√†o Power BI:
1. M·ªü Power BI Desktop
2. Get Data ‚Üí Text/CSV
3. Navigate to `powerbi_data/` folder
4. Import c√°c CSV files
5. T·∫°o relationships theo date columns
6. Use DAX measures t·ª´ `DAX_Measures.txt`

## üêõ Troubleshooting

### L·ªói Spark tr√™n Windows:
```bash
# COMMON ERROR: TypeError: 'JavaPackage' object is not callable
# ƒê√¢y l√† l·ªói ph·ªï bi·∫øn v·ªõi PySpark tr√™n Windows

# GI·∫¢I PH√ÅP 1: S·ª≠ d·ª•ng standalone mode (KHUY·∫æN NGH·ªä)
python test_enhanced_real.py

# GI·∫¢I PH√ÅP 2: Downgrade PySpark
pip uninstall pyspark
pip install pyspark==3.4.0

# GI·∫¢I PH√ÅP 3: Set JAVA_HOME (n·∫øu c√≥ Java)
# set JAVA_HOME=C:\Program Files\Java\jdk-11.0.x
# set PATH=%JAVA_HOME%\bin;%PATH%

# GI·∫¢I PH√ÅP 4: S·ª≠ d·ª•ng WSL (Windows Subsystem for Linux)
wsl
python main.py --stage full --enhanced
```

**‚ö†Ô∏è L∆ØU √ù**: Tr√™n Windows, khuy·∫øn ngh·ªã s·ª≠ d·ª•ng `test_enhanced_real.py` thay v√¨ `main.py` ƒë·ªÉ tr√°nh Spark issues.

### L·ªói Missing Dependencies:
```bash
# C√†i th√™m packages
pip install pyarrow         # Cho Parquet support
pip install openpyxl        # Cho Excel support
pip install plotly          # Cho Prophet plots
```

### L·ªói Memory:
```bash
# Gi·∫£m data size trong config
FORECAST_HORIZON = 7        # Thay v√¨ 30
TEST_DAYS = 7              # Thay v√¨ 14
```

## üîß Customization

### Th√™m Model m·ªõi:
```python
# Trong forecasting.py
def your_model_forecast(self, train_data, test_data):
    # Your model logic here
    return test_pred, future_pred

# Th√™m v√†o pipeline
results.append({
    "model": "your_model",
    "rmse": self.rmse(y_test, test_pred),
    "mape": self.mape(y_test, test_pred)
})
```

### Th√™m Features m·ªõi:
```python
# Trong create_features()
def create_features(self, df):
    # Existing features...
    
    # Your custom features
    df["weekend"] = (df.index.dayofweek >= 5).astype(int)
    df["season"] = df.index.month % 12 // 3 + 1
    return df
```

## üìö Best Practices

### 1. Data Quality:
- Always validate input data schema
- Handle missing values appropriately
- Log data quality metrics
- Monitor outliers

### 2. Performance:
- Use Parquet format for large datasets
- Partition data by date for better performance
- Cache frequently used DataFrames
- Use appropriate Spark configurations

### 3. Model Management:
- Track model performance metrics
- Save model artifacts for reproducibility
- Use cross-validation for model selection
- Monitor model drift in production

### 4. Monitoring:
- Log all pipeline steps
- Set up alerts for failures
- Monitor data freshness
- Track business KPIs

## üéØ Production Deployment

### Databricks:
1. Upload notebooks to Databricks workspace
2. Create cluster with appropriate configurations
3. Schedule jobs using Databricks Jobs
4. Set up monitoring and alerting

### Local Scheduler:
```bash
# S·ª≠ d·ª•ng cron (Linux/Mac) ho·∫∑c Task Scheduler (Windows)
# Ch·∫°y pipeline h√†ng ng√†y l√∫c 2AM
0 2 * * * /path/to/iot_env/bin/python /path/to/main.py --stage full
```

## üîó T√†i li·ªáu tham kh·∫£o

- [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/)
- [Prophet Documentation](https://facebook.github.io/prophet/)
- [XGBoost Documentation](https://xgboost.readthedocs.io/)
- [Power BI Documentation](https://docs.microsoft.com/en-us/power-bi/)

---

**üéâ Ch√∫c b·∫°n build pipeline th√†nh c√¥ng!** 

N·∫øu g·∫∑p v·∫•n ƒë·ªÅ, h√£y check logs trong `pipeline.log` v√† follow troubleshooting guide tr√™n.